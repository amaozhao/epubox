import json
import logging
import os
import re
import sys
from typing import Dict

import ebooklib
import nltk
from bs4 import BeautifulSoup
from ebooklib import epub
from nltk import pos_tag, word_tokenize
from nltk.chunk import RegexpParser
from sklearn.feature_extraction.text import TfidfVectorizer

from .utils import CODE_KEYWORDS, GENERIC_BLACKLIST, INVALID_CHARS


class GlossaryExtractor:
    """
    [ç»ˆæç‰ˆ] ä¸€ä¸ªç”¨äºä»EPUBæ–‡ä»¶ä¸­æå–æŠ€æœ¯æœ¯è¯­å¹¶ç”Ÿæˆæœ¯è¯­è¡¨è‰æ¡ˆçš„ç±»ã€‚
    é‡‡ç”¨ã€NLTKåè¯çŸ­è¯­åˆ†å— + TF-IDFæƒé‡æ’åº + ç»ˆæè¿‡æ»¤ã€‘çš„æ–¹æ¡ˆã€‚
    """

    # --- è¿‡æ»¤è§„åˆ™é…ç½® (ç»ˆæç‰ˆ) ---
    MIN_WORDS = 2
    MAX_WORDS = 5

    # 1. ä»»ä½•åŒ…å«è¿™äº›å­—ç¬¦çš„æœ¯è¯­éƒ½å°†è¢«è§†ä¸ºä»£ç æˆ–åƒåœ¾
    INVALID_CHARS = INVALID_CHARS

    # 2. ä»»ä½•åŒ…å«è¿™äº›ç¼–ç¨‹å…³é”®è¯çš„æœ¯è¯­ï¼Œéƒ½å°†è¢«è§†ä¸ºä»£ç 
    CODE_KEYWORDS = CODE_KEYWORDS

    # 3. æœ€ç»ˆã€æœ€å…¨é¢çš„é€šç”¨/ç¤ºä¾‹è¯é»‘åå•
    GENERIC_BLACKLIST = GENERIC_BLACKLIST

    def _ensure_nltk_data(self):
        """ç¡®ä¿è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰NLTKæ•°æ®åŒ…éƒ½å·²ä¸‹è½½ã€‚"""
        required_packages = ["punkt", "stopwords", "averaged_perceptron_tagger"]
        try:
            for package in required_packages:
                nltk.download(package, quiet=True, raise_on_error=True)
        except Exception as e:
            logging.error(f"âŒ ä¸‹è½½NLTKæ ¸å¿ƒæ•°æ®åŒ…æ—¶å¤±è´¥: {e}", exc_info=False)
            logging.error("   è¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥ã€‚ç¨‹åºæ— æ³•ç»§ç»­ã€‚")
            sys.exit(1)

    def __init__(self):
        logging.info("æ­£åœ¨åˆå§‹åŒ– GlossaryExtractor (æ–¹æ¡ˆ: ç»ˆæç‰ˆ)...")
        self._ensure_nltk_data()
        stop_words_set = set(nltk.corpus.stopwords.words("english"))
        self.forbidden_words = stop_words_set.union(self.GENERIC_BLACKLIST)
        self.grammar = r"NP: {<JJ.*>*<NN.*>+}"
        self.chunker = RegexpParser(self.grammar)
        logging.info("âœ… Extractor åˆå§‹åŒ–æˆåŠŸã€‚")

    def _is_valid_term(self, term: str) -> bool:
        """å¯¹å•ä¸ªå€™é€‰æœ¯è¯­è¿›è¡Œå¤šå±‚å¼ºåŠ›è§„åˆ™æ ¡éªŒã€‚"""
        words = term.split()
        if not (self.MIN_WORDS <= len(words) <= self.MAX_WORDS):
            return False
        if any(char in term for char in self.INVALID_CHARS):
            return False
        term_words_set = set(words)
        if not term_words_set.isdisjoint(self.CODE_KEYWORDS):
            return False
        if not re.search(r"[a-zA-Z]", term) or not term[0].isalnum() or not term[-1].isalnum():
            return False
        if term_words_set.issubset(self.forbidden_words):
            return False
        return True

    def _extract_text_from_epub(self, epub_path: str) -> list[str] | None:
        """ä»EPUBä¸­æå–å¹¶å‡€åŒ–æ–‡æœ¬å†…å®¹ã€‚"""
        logging.info("ğŸ“– [é˜¶æ®µ1/3] æ­£åœ¨ä»EPUBä¸­è§£æå’Œå‡€åŒ–HTMLå†…å®¹...")
        try:
            book = epub.read_epub(epub_path)
        except Exception as e:
            logging.error(f"âŒ è¯»å–EPUBæ–‡ä»¶ '{epub_path}' å¤±è´¥: {e}")
            return None
        documents, tags_to_ignore = (
            [],
            ["pre", "code", "figure", "figcaption", "table", "script", "style", "a", "header", "footer", "nav"],
        )
        for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):
            soup = BeautifulSoup(item.get_content(), "html.parser")
            for bad_tag in soup(tags_to_ignore):
                bad_tag.decompose()
            text_content = re.sub(r"\s+", " ", soup.get_text(separator=" "))
            if text_content and len(text_content.strip()) > 100:
                documents.append(text_content.strip())
        if not documents:
            logging.warning("âš ï¸ æœªèƒ½ä»EPUBä¸­æå–ä»»ä½•æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹ã€‚")
            return None
        logging.info(f"   æˆåŠŸè§£æå¹¶æ¸…ç†äº† {len(documents)} ä¸ªæ–‡æ¡£ï¼ˆç« èŠ‚ï¼‰ã€‚")
        return documents

    def _get_all_unique_terms(self, documents: list[str], top_n: int = 200) -> list[str]:
        """ç»“åˆåè¯çŸ­è¯­æå–ã€TF-IDFè¯„åˆ†å’Œå¼ºåŠ›è§„åˆ™è¿‡æ»¤ã€‚"""
        logging.info("ğŸ” [é˜¶æ®µ2/3] æ­£åœ¨æå–å€™é€‰çŸ­è¯­å¹¶è¿›è¡Œå¼ºåŠ›è¿‡æ»¤...")
        full_text = " ".join(documents)
        sentences = nltk.sent_tokenize(full_text)
        if not sentences:
            return []
        candidate_phrases = set()
        for sentence in sentences:
            try:
                tokens = word_tokenize(sentence)
                pos_tags = pos_tag(tokens)
                chunked = self.chunker.parse(pos_tags)
                for subtree in chunked.subtrees(filter=lambda t: t.label() == "NP"):
                    phrase = " ".join(word for word, tag in subtree.leaves()).lower()
                    if self._is_valid_term(phrase):
                        candidate_phrases.add(phrase)
            except Exception:
                continue
        if not candidate_phrases:
            logging.warning("   âš ï¸ å¼ºåŠ›è¿‡æ»¤åæœªèƒ½æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å€™é€‰æœ¯è¯­ã€‚")
            return []
        logging.info(f"   è¿‡æ»¤åå‰©ä¸‹ {len(candidate_phrases)} ä¸ªé«˜è´¨é‡å€™é€‰ã€‚")
        logging.info("ğŸ” [é˜¶æ®µ3/3] æ­£åœ¨ä¸ºé«˜è´¨é‡å€™é€‰è®¡ç®—TF-IDFæƒé‡å¹¶æ’åº...")
        vectorizer = TfidfVectorizer(vocabulary=list(candidate_phrases), stop_words="english")
        try:
            tfidf_matrix = vectorizer.fit_transform(sentences)
        except ValueError:
            return []
        scores = tfidf_matrix.mean(axis=0).A1
        scored_phrases = {phrase: score for phrase, score in zip(vectorizer.get_feature_names_out(), scores)}
        sorted_phrases = sorted(scored_phrases.items(), key=lambda x: x[1], reverse=True)
        final_terms = [term.title() for term, score in sorted_phrases[:top_n]]
        logging.info(f"   æœ€ç»ˆç»“æœ: ç­›é€‰å‡º Top {len(final_terms)} ä¸ªé«˜è´¨é‡æœ¯è¯­ã€‚")
        return sorted(final_terms)

    def run(self, epub_path: str, output_path: str | None = None):
        """æ‰§è¡Œå®Œæ•´çš„æœ¯è¯­æå–æµç¨‹ã€‚"""
        logging.info(f"ğŸš€ å¼€å§‹å¤„ç†EPUBæ–‡ä»¶: {os.path.basename(epub_path)}")
        if output_path is None:
            glossary_dir = "glossary"
            os.makedirs(glossary_dir, exist_ok=True)
            base_name = os.path.splitext(os.path.basename(epub_path))[0]
            output_path = os.path.join(glossary_dir, f"{base_name}.json")
            logging.info(f"â„¹ï¸ æœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œå°†è‡ªåŠ¨ä½¿ç”¨: '{output_path}'")
        else:
            output_dir = os.path.dirname(str(output_path))
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
        documents = self._extract_text_from_epub(epub_path)
        if not documents:
            return
        all_terms = self._get_all_unique_terms(documents)
        if not all_terms:
            logging.warning("âš ï¸ æœªèƒ½ç”Ÿæˆå€™é€‰æœ¯è¯­åˆ—è¡¨ã€‚æµç¨‹ç»ˆæ­¢ã€‚")
            return
        existing_glossary = {}
        if os.path.exists(output_path):
            try:
                with open(output_path, "r", encoding="utf-8") as f:
                    existing_glossary = json.load(f)
                logging.info(f"ğŸ”„ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æœ¯è¯­è¡¨ï¼Œå…± {len(existing_glossary)} æ¡ã€‚")
            except (json.JSONDecodeError, IOError) as e:
                logging.error(f"âŒ åŠ è½½ç°æœ‰æœ¯è¯­è¡¨ '{output_path}' å¤±è´¥: {e}ã€‚")
        final_glossary = {term: "" for term in all_terms}
        restored_count = 0
        for term, translation in existing_glossary.items():
            if term.title() in final_glossary and translation:
                final_glossary[term.title()] = translation
                restored_count += 1
        if restored_count > 0:
            logging.info(f"   æ¢å¤äº† {restored_count} æ¡å·²æœ‰çš„ç¿»è¯‘ã€‚")
        existing_keys = {k.title() for k in existing_glossary.keys()}
        final_keys = set(final_glossary.keys())
        added_count = len(final_keys - existing_keys)
        removed_count = len(existing_keys - final_keys)
        if added_count == 0 and removed_count == 0 and restored_count == len(final_glossary):
            logging.info("âœ… æœ¯è¯­è¡¨å†…å®¹ä¸ä¹¦ä¸­æå–ç»“æœä¸€è‡´ï¼Œæ— éœ€æ›´æ–°ã€‚")
        else:
            if added_count > 0:
                logging.info(f"â• æ–°å¢äº† {added_count} ä¸ªå…¨æ–°çš„æœ¯è¯­ã€‚")
            if removed_count > 0:
                logging.info(f"ğŸ—‘ï¸ æœ¯è¯­è¡¨å·²æ¸…ç†ï¼Œç§»é™¤äº† {removed_count} ä¸ªè¿‡æ—¶çš„æœ¯è¯­ã€‚")
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(dict(sorted(final_glossary.items())), f, indent=4, ensure_ascii=False)
            logging.info(f"\nâœ… æœ¯è¯­è¡¨å·²æˆåŠŸæ›´æ–°å¹¶ä¿å­˜åˆ°: '{output_path}'")
        except IOError as e:
            logging.error(f"âŒ æ— æ³•å†™å…¥æ–‡ä»¶ '{output_path}': {e}")


class GlossaryLoader:
    def __init__(self, glossary_dir: str = "glossary"):
        self.glossary_dir = glossary_dir
        if not os.path.isdir(glossary_dir):
            logging.warning(f"âš ï¸ æœ¯è¯­è¡¨ç›®å½• '{glossary_dir}' ä¸å­˜åœ¨ã€‚")

    def load(self, epub_path: str) -> Dict[str, str]:
        base_name = os.path.splitext(os.path.basename(epub_path))[0]
        glossary_path = os.path.join(self.glossary_dir, f"{base_name}.json")
        logging.info(f"ğŸ“‚ æ­£åœ¨å°è¯•ä» '{glossary_path}' åŠ è½½æœ¯è¯­è¡¨...")
        if not os.path.exists(glossary_path):
            logging.warning("   æœ¯è¯­è¡¨æ–‡ä»¶ä¸å­˜åœ¨ã€‚å°†ä½¿ç”¨ç©ºæœ¯è¯­è¡¨ã€‚")
            return {}
        try:
            with open(glossary_path, "r", encoding="utf-8") as f:
                glossary_data = json.load(f)
            translated_glossary = {k: v for k, v in glossary_data.items() if v}
            logging.info(f"   æˆåŠŸåŠ è½½å¹¶è¿‡æ»¤äº† {len(translated_glossary)} æ¡å·²ç¿»è¯‘çš„æœ¯è¯­ã€‚")
            return translated_glossary
        except (json.JSONDecodeError, IOError) as e:
            logging.error(f"âŒ åŠ è½½æˆ–è§£ææœ¯è¯­è¡¨ '{glossary_path}' å¤±è´¥: {e}")
            return {}
